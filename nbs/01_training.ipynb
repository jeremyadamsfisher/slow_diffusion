{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Algorithm\n",
    "\n",
    "> Set up the basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremy/micromamba/envs/slowai/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import ast\n",
    "from typing import Sequence\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from beartype import beartype\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from slow_diffusion.model import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class UnetLightning(L.LightningModule):\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        nfs: Sequence[int],\n",
    "        n_blocks: Sequence[int],\n",
    "        color_channels: int,\n",
    "        lr: float = 4e-3,\n",
    "        one_cycle_pct_start: float = 0.3,\n",
    "        one_cycle_div_factor: float = 25.0,\n",
    "        one_cycle_final_div_factor: float = 1e4,\n",
    "        adamw_epsilon: float = 1e-5,\n",
    "        kaiming: bool = False,\n",
    "    ):\n",
    "        \"\"\"Unet training code\n",
    "\n",
    "        Args:\n",
    "            nfs:  Number of channels in a {Up,Down}block\n",
    "            n_blocks: Number of sub-blocks in {Up,Down}block. Should have 1\n",
    "                more entry than `nfs`\n",
    "            color_channels: Color channels, or however many dimensions in the\n",
    "                VAE bottleneck space\n",
    "            lr: learning rate\n",
    "            one_cycle_pct_start: The percentage of the cycle (in number of\n",
    "                steps) spent increasing the learning rate. Default: 0.3\n",
    "            one_cycle_div_factor:  Determines the initial learning rate\n",
    "                via initial_lr = max_lr/div_factor Default: 25\n",
    "            one_cycle_final_div_factor: Determines the minimum learning rate\n",
    "                via min_lr = initial_lr/final_div_factor Default: 1e4\n",
    "            adamw_epsilon:  term added to the denominator to improve numerical\n",
    "                stability. PyTorch defaults to 1e-8. 1e-5 is better.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.unet = Unet.kaiming(\n",
    "            nfs=nfs, n_blocks=n_blocks, color_channels=color_channels\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def step(self, batch):\n",
    "        (x_t, t), epsilon = batch\n",
    "        preds = self.unet(x_t, t)\n",
    "        return self.loss_fn(preds, epsilon)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch)\n",
    "        self.log(\"train_loss\", loss, on_step=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch)\n",
    "        self.log(\"test_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            eps=self.hparams.adamw_epsilon,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": torch.optim.lr_scheduler.OneCycleLR(\n",
    "                    optimizer,\n",
    "                    max_lr=self.hparams.lr,\n",
    "                    total_steps=self.trainer.estimated_stepping_batches,\n",
    "                    pct_start=self.hparams.one_cycle_pct_start,\n",
    "                    div_factor=self.hparams.one_cycle_div_factor,\n",
    "                    final_div_factor=self.hparams.one_cycle_final_div_factor,\n",
    "                ),\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,  # Update the LR every step\n",
    "                \"monitor\": \"test_loss\",  # Not relevant for OneCycleLR but specified anyways\n",
    "                \"strict\": True,  # FYI doesn't need to be strict because the monitor is irrelevant\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this for testing downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "def get_tiny_unet():\n",
    "    return UnetLightning(\n",
    "        nfs=(224, 448, 672, 896),\n",
    "        n_blocks=(3, 2, 2, 1, 1),\n",
    "        color_channels=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowAI",
   "language": "python",
   "name": "slowai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
