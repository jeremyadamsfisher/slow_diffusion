{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "\n",
    "> Monitor different aspects of the model and training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# |export\n",
    "import re\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "from glom import glom\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch import nn\n",
    "\n",
    "import wandb\n",
    "from slow_diffusion.fashionmnist import FashionMNISTDataModule\n",
    "from slow_diffusion.training import get_tiny_unet_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure the model can be inspected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_run(callback):\n",
    "    with wandb.init():\n",
    "        dm = FashionMNISTDataModule(256, n_workers=0)\n",
    "        dm.setup()\n",
    "        model = get_tiny_unet_lightning()\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=1,\n",
    "            callbacks=[callback],\n",
    "            logger=WandbLogger(),\n",
    "            precision=\"bf16-mixed\",\n",
    "            log_every_n_steps=1,\n",
    "        )\n",
    "        trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class MonitorCallback(L.Callback):\n",
    "    \"\"\"Log arbitrary properties in the training run, such as LR.\"\"\"\n",
    "\n",
    "    def __init__(self, gloms: dict[str, str]):\n",
    "        super().__init__()\n",
    "        self.gloms = gloms\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        args = Namespace(\n",
    "            trainer=trainer,\n",
    "            pl_module=pl_module,\n",
    "            outputs=outputs,\n",
    "            batch=batch,\n",
    "            batch_idx=batch_idx,\n",
    "        )\n",
    "        for name, spec in self.gloms.items():\n",
    "            self.log(name, glom(args, spec), on_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjfisher40\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jeremiahfisher/Code/slow_diffusion/nbs/wandb/run-20240825_112257-86jhcqof</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/86jhcqof' target=\"_blank\">jolly-pine-70</a></strong> to <a href='https://wandb.ai/jfisher40/slow_diffusion-nbs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jfisher40/slow_diffusion-nbs' target=\"_blank\">https://wandb.ai/jfisher40/slow_diffusion-nbs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/86jhcqof' target=\"_blank\">https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/86jhcqof</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | unet    | Unet    | 15.7 M | train\n",
      "1 | loss_fn | MSELoss | 0      | train\n",
      "--------------------------------------------\n",
      "15.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.7 M    Total params\n",
      "62.964    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                      | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_run(MonitorCallback({\"lr\": \"trainer.optimizers.0.param_groups.0.lr\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class CountDeadUnitsCallback(L.Callback):\n",
    "    \"\"\"Check for numeric underflow or overflow\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        nans = 0\n",
    "        zeros = 0\n",
    "        for _, params in pl_module.named_parameters():\n",
    "            nans += params.isnan().int().sum().item()\n",
    "            zeros += (params == 0).sum().item()\n",
    "        self.log(\"nans\", nans, reduce_fx=max)\n",
    "        self.log(\"zeros\", zeros, reduce_fx=max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_run(CountDeadUnitsCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check activation distribution metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class Stats:\n",
    "    def __init__(self, label, module, log, live):\n",
    "        self.label = label\n",
    "        self.hook = module.register_forward_hook(self.append)\n",
    "        self.log = log\n",
    "        self.live = live\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "\n",
    "    def append(self, module, _, activations):\n",
    "        if not module.training:\n",
    "            return\n",
    "        activations = activations.cpu()\n",
    "        mean = activations.mean().cpu().item()\n",
    "        std = activations.std().cpu().item()\n",
    "        if self.live:\n",
    "            self.log(f\"{self.label}:mean\", mean)\n",
    "            self.log(f\"{self.label}:std\", std)\n",
    "        else:\n",
    "            self.means.append(mean)\n",
    "            self.stds.append(std)\n",
    "\n",
    "    def plot(self, ax0, ax1):\n",
    "        ax0.plot(self.means)\n",
    "        ax1.plot(self.stds, label=self.label)\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "class StatsCallback(L.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mods: list[type[nn.Module]] | None = None,\n",
    "        mod_filter: str | None = None,\n",
    "        live=False,\n",
    "    ):\n",
    "        assert mods or mod_filter\n",
    "        self.mods = []\n",
    "        if mods is not None:\n",
    "            self.mods.extend(mods)\n",
    "        self.mod_filter = mod_filter\n",
    "        self.mod_stats = []\n",
    "        self.live = live\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        c = Counter()\n",
    "        for mod in self.mods:\n",
    "            cls_name = mod.__class__.__name__\n",
    "            name = f\"{cls_name}:{c.get(cls_name)}\"\n",
    "            s = Stats(name, mod, self.log, self.live)\n",
    "            self.mod_stats.append(s)\n",
    "            c.update((cls_name,))\n",
    "\n",
    "        if self.mod_filter is not None:\n",
    "            for name, mod in pl_module.named_modules():\n",
    "                if re.match(self.mod_filter, name):\n",
    "                    s = Stats(name, mod, self.log, self.live)\n",
    "                    self.mod_stats.append(s)\n",
    "\n",
    "    def plot(self, log=True):\n",
    "        with plt.style.context(\"ggplot\"):\n",
    "            fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "            ax0.set(title=\"Means\", xlabel=\"Time Step\", ylabel=\"Activation\")\n",
    "            ax1.set(title=\"STDs\", xlabel=\"Time Step\")\n",
    "            for mod_stat in self.mod_stats:\n",
    "                mod_stat.plot(ax0, ax1)\n",
    "            fig.legend(loc=7)\n",
    "            fig.subplots_adjust(right=0.75)\n",
    "            return fig\n",
    "\n",
    "    def log_stats(self):\n",
    "        if not self.live:\n",
    "            fig = self.plot()\n",
    "            img = wandb.Image(fig)\n",
    "            wandb.log({\"stats\": img})\n",
    "            plt.close(fig)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        self.log_stats()\n",
    "\n",
    "    def cleanup(self):\n",
    "        for s in self.mod_stats:\n",
    "            s.cleanup()\n",
    "\n",
    "    def on_fit_end(self, trainer, pl_module):\n",
    "        self.log_stats()\n",
    "        self.cleanup()\n",
    "\n",
    "    def on_exception(self, trainer, pl_module, exception):\n",
    "        self.log_stats()\n",
    "        self.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cb = StatsCallback(mod_filter=r\"unet.(((down|up)blocks.\\d+)|start|middle|end)(?!\\.)\")\n",
    "test_run(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how _bad_ the training dynamics are initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowDiffusion",
   "language": "python",
   "name": "slow_diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
