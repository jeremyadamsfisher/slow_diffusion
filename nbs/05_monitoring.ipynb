{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "\n",
    "> Monitor different aspects of the model and training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# |export\n",
    "import re\n",
    "from argparse import Namespace\n",
    "\n",
    "import lightning as L\n",
    "from glom import glom\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from torch import nn\n",
    "\n",
    "import wandb\n",
    "from slow_diffusion.fashionmnist import FashionMNISTDataModule\n",
    "from slow_diffusion.training import get_tiny_unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure the model can be inspected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fashion_test_run(callback):\n",
    "    with wandb.init():\n",
    "        wandb_logger = WandbLogger()\n",
    "        dm = FashionMNISTDataModule(256, n_workers=0)\n",
    "        dm.setup()\n",
    "        model = get_tiny_unet()\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=1,\n",
    "            callbacks=[],\n",
    "            logger=WandbLogger(),\n",
    "            precision=\"bf16-mixed\",\n",
    "        )\n",
    "        trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log arbitrary properties in the training run, such as LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "class MonitorCallback(L.Callback):\n",
    "    def __init__(self, gloms: dict[str, str]):\n",
    "        super().__init__()\n",
    "        if not gloms:\n",
    "            raise ValueError\n",
    "        self.gloms = gloms\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        args = Namespace(\n",
    "            trainer=trainer,\n",
    "            pl_module=pl_module,\n",
    "            outputs=outputs,\n",
    "            batch=batch,\n",
    "            batch_idx=batch_idx,\n",
    "        )\n",
    "        for name, spec in self.gloms.items():\n",
    "            self.log(name, glom(args, spec), on_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjfisher40\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jeremiahfisher/Code/slow_diffusion/nbs/wandb/run-20240818_104405-p93tljj7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/p93tljj7' target=\"_blank\">celestial-planet-20</a></strong> to <a href='https://wandb.ai/jfisher40/slow_diffusion-nbs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jfisher40/slow_diffusion-nbs' target=\"_blank\">https://wandb.ai/jfisher40/slow_diffusion-nbs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/p93tljj7' target=\"_blank\">https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/p93tljj7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:01<00:00, 6954.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████████████| 60000/60000 [00:00<00:00, 229322.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|███████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 190447.61 examples/s]\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | unet    | Unet    | 185 M  | train\n",
      "1 | loss_fn | MSELoss | 0      | train\n",
      "--------------------------------------------\n",
      "185 M     Trainable params\n",
      "0         Non-trainable params\n",
      "185 M     Total params\n",
      "742.125   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                                                                                  | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                    | 0/235 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "fashion_test_run(MonitorCallback({\"lr\": \"trainer.optimizers.0.param_groups.0.lr\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class CountDeadUnitsCallback(L.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        nans = 0\n",
    "        for _, params in pl_module.named_parameters():\n",
    "            nans += params.isnan().int().sum().item()\n",
    "        self.log(\"dead_units\", nans, reduce_fx=sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountDeadUnitsCallback(),\n",
    "StatsCallback(mod_filter=r\"convs\"),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check activation distribution metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class Stats:\n",
    "    def __init__(self, label, module):\n",
    "        self.label = label\n",
    "        self.hook = module.register_forward_hook(self.append)\n",
    "\n",
    "    def append(self, module, _, activations):\n",
    "        if not module.training:\n",
    "            return\n",
    "        activations = activations.cpu()\n",
    "        self.log(f\"{self.label}:mean\", activations.mean().cpu().item())\n",
    "        self.log(f\"{self.label}:std\", activations.std().cpu().item())\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "class StatsCallback(L.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mods: list[type[nn.Module]] | None = None,\n",
    "        mod_filter: str | None = None,\n",
    "    ):\n",
    "        assert mods or mod_filter\n",
    "        self.mods = []\n",
    "        if mods is not None:\n",
    "            self.mods.extend(mods)\n",
    "        self.mod_filter = mod_filter\n",
    "        self.mod_stats = []\n",
    "\n",
    "    def on_fit_start(self, trainer, pl_module):\n",
    "        if self.mod_filter is not None:\n",
    "            for name, mod in pl_module.named_modules():\n",
    "                if re.match(self.mod_filter, name):\n",
    "                    self.mods.append(mod)\n",
    "\n",
    "        for i, mod in self.mods:\n",
    "            s = Stats(f\"layer_{i}\", mod)\n",
    "            self.mod_stats.append(s)\n",
    "\n",
    "    def cleanup(self):\n",
    "        for s in self.mod_stats:\n",
    "            s.cleanup()\n",
    "\n",
    "    def on_fit_end(self, trainer, pl_module):\n",
    "        self.cleanup()\n",
    "\n",
    "    def on_exception(self, trainer, pl_module, exception):\n",
    "        self.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init():\n",
    "    wandb_logger = WandbLogger()\n",
    "    dm = FashionMNISTDataModule(256, n_workers=0)\n",
    "    dm.setup()\n",
    "    model = get_tiny_unet()\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=1,\n",
    "        callbacks=[\n",
    "            MonitorCallback({\"lr\": \"trainer.optimizers.0.param_groups.0.lr\"}),\n",
    "            CountDeadUnitsCallback(),\n",
    "            StatsCallback(mod_filter=r\"convs\"),\n",
    "        ],\n",
    "        logger=WandbLogger(),\n",
    "        precision=\"bf16-mixed\",\n",
    "    )\n",
    "    trainer.fit(model=model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowDiffusion",
   "language": "python",
   "name": "slow_diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
