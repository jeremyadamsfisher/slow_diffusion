{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Algorithm\n",
    "\n",
    "> Set up the basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Sequence\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from slow_diffusion.model import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class UnetConfig(BaseModel):\n",
    "    \"\"\"Model configuation\"\"\"\n",
    "\n",
    "    # Number of channels in a {Up,Down}block\n",
    "    nfs: Sequence[int]\n",
    "\n",
    "    # Number of sub-blocks in {Up,Down}block. Should have 1 more entry than `nfs`\n",
    "    n_blocks: Sequence[int]\n",
    "\n",
    "    # Color channels, or however many dimensions in the VAE bottleneck space\n",
    "    color_channels: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class TrainingConfig(BaseModel):\n",
    "    \"\"\"Training configuation\"\"\"\n",
    "\n",
    "    lr: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class Config(BaseModel):\n",
    "    \"\"\"Top-level configuation\"\"\"\n",
    "\n",
    "    unet_config: UnetConfig\n",
    "    training_config: TrainingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class UnetLightning(L.LightningModule):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.unet = Unet(**config.unet_config.dict())\n",
    "        self.save_hyperparameters(config.dict())\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def step(self, batch):\n",
    "        (x_t, t), epsilon = learn.batch\n",
    "        preds = self.unet(x_t, t)\n",
    "        return self.loss_fn(preds, epsilon)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch)\n",
    "        self.log(\"trn_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch)\n",
    "        self.log(\"tst_loss\", loss, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.lr)\n",
    "        if self.config.one_cycle_scheduler is False:\n",
    "            return optimizer\n",
    "        else:\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": torch.optim.lr_scheduler.OneCycleLR(\n",
    "                        optimizer,\n",
    "                        max_lr=self.config.lr,\n",
    "                        total_steps=self.trainer.estimated_stepping_batches,\n",
    "                        pct_start=self.config.one_cycle_config.pct_start,\n",
    "                        div_factor=self.config.one_cycle_config.div_factor,\n",
    "                        final_div_factor=self.config.one_cycle_config.final_div_factor,\n",
    "                    ),\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,  # Update the LR every step\n",
    "                    \"monitor\": \"tst_loss\",  # Not relevant for OneCycleLR\n",
    "                    \"strict\": True,  # Doesn't need to be strict because the monitor is irrelevant\n",
    "                },\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    unet_config=UnetConfig(\n",
    "        nfs=(224, 448, 672, 896),\n",
    "        n_blocks=(3, 2, 2, 1, 1),\n",
    "        color_channels=3,\n",
    "    ),\n",
    "    training_config=TrainingConfig(lr=4e-3),\n",
    ")\n",
    "unet = UnetLightning(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slow_diffusion",
   "language": "python",
   "name": "slow_diffusion"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
