{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "> Central diffusion model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is adapted from my [FastAI notes]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import math\n",
    "from typing import Sequence, TypeAlias\n",
    "\n",
    "import torch\n",
    "from beartype import beartype\n",
    "from jaxtyping import Float, jaxtyped\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "InFeatureMapTensor: TypeAlias = Float[Tensor, \"bs c_in h_in w_in\"]\n",
    "OutFeatureMapTensor: TypeAlias = Float[Tensor, \"bs c_out h_out w_out\"]\n",
    "TimeStepTensor: TypeAlias = Float[Tensor, \"bs\"]  # from 0 to 1\n",
    "TimeStepEmbeddingTensor: TypeAlias = Float[Tensor, \"bs t\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules\n",
    "\n",
    "Now, let's get to the module definitions.\n",
    "\n",
    "The ConvBlock is laid out in the \"Preactivation\" configuration, like so:\n",
    "\n",
    "![](https://www.researchgate.net/publication/337691625/figure/fig2/AS:831842160746496@1575338034408/Architecture-of-normal-residual-block-a-and-pre-activation-residual-block-b.jpg)\n",
    "\n",
    "This does mean that we need to take care in the first block to use a \"raw\" nn.Conv, because otherwise the activation would discard pixel information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Wrapper for a Conv block with normalization and activation\"\"\"\n",
    "\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_in: int,\n",
    "        c_out: int,\n",
    "        ks: int = 3,\n",
    "        stride: int = 1,\n",
    "        act: type[nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ks = ks\n",
    "        self.norm = nn.BatchNorm2d(c_in)\n",
    "        self.act = act()\n",
    "        self.conv = nn.Conv2d(\n",
    "            c_in,\n",
    "            c_out,\n",
    "            stride=stride,\n",
    "            kernel_size=ks,\n",
    "            padding=ks // 2,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(self, x: InFeatureMapTensor) -> OutFeatureMapTensor:\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "@jaxtyped(typechecker=beartype)\n",
    "def timestep_embedding(\n",
    "    ts: TimeStepTensor, emb_dim: int, max_period: int = 10_000\n",
    ") -> TimeStepEmbeddingTensor:\n",
    "    exponent = -math.log(max_period) * torch.linspace(\n",
    "        0, 1, emb_dim // 2, device=ts.device\n",
    "    )\n",
    "    embedding = ts[:, None].float() * exponent.exp()[None, :]\n",
    "    embedding = torch.cat([embedding.sin(), embedding.cos()], dim=-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class TimeEmbeddingMixer(nn.Module):\n",
    "    \"\"\"Incorporate the time embedding into the ResBlock logits\"\"\"\n",
    "\n",
    "    @beartype\n",
    "    def __init__(self, c_time: int, c_out: int, act: type[nn.Module] = nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(c_time, c_out * 2)\n",
    "        self.act = act()\n",
    "\n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(\n",
    "        self, x: InFeatureMapTensor, t_emb: TimeStepEmbeddingTensor\n",
    "    ) -> OutFeatureMapTensor:\n",
    "        t_emb = self.lin(self.act(t_emb))[:, :, None, None]\n",
    "        scale, shift = torch.chunk(t_emb, 2, dim=1)\n",
    "        return x * (1 + scale) + shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class ResBlock(nn.Module):\n",
    "    \"\"\"Conv resblock with the preactivation configuration and time embedding modulation\"\"\"\n",
    "\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_time: int,\n",
    "        c_in: int,\n",
    "        c_out: int,\n",
    "        ks: int = 3,\n",
    "        stride: int = 2,\n",
    "        act: type[nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.c_time = c_time\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "\n",
    "        self.time_mixer = TimeEmbeddingMixer(c_time, c_out, act=act)\n",
    "        self.conv_a = ConvBlock(c_in, c_out, act=act)\n",
    "        self.conv_b = ConvBlock(c_out, c_out, act=act)\n",
    "        if c_in != c_out:\n",
    "            self.id_conv = nn.Conv2d(c_in, c_out, kernel_size=1)\n",
    "        else:\n",
    "            self.id_conv = None\n",
    "\n",
    "        # Used for the Unet cross-link\n",
    "        self.output = None\n",
    "\n",
    "    def non_residual(self, x, t_emb):\n",
    "        x = self.conv_a(x)\n",
    "        x = self.time_mixer(x, t_emb)\n",
    "        x = self.conv_b(x)\n",
    "        return x\n",
    "\n",
    "    def residual(self, x):\n",
    "        if self.id_conv is not None:\n",
    "            return self.id_conv(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(\n",
    "        self, x: InFeatureMapTensor, t_emb: TimeStepEmbeddingTensor\n",
    "    ) -> OutFeatureMapTensor:\n",
    "        x = self.non_residual(x, t_emb) + self.residual(x)\n",
    "        self.output = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = torch.randn(16, 3, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = xb.shape[0]\n",
    "ts: TimeStepTensor = torch.linspace(-10, 10, bs)\n",
    "tse = timestep_embedding(ts, 32)\n",
    "tse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.0000,  -8.6667,  -7.3333,  -6.0000,  -4.6667,  -3.3333,  -2.0000,\n",
       "         -0.6667,   0.6667,   2.0000,   3.3333,   4.6667,   6.0000,   7.3333,\n",
       "          8.6667,  10.0000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class Downblock(nn.Module):\n",
    "    \"\"\"A superblock consisting of many downblocks of similar resolutions\"\"\"\n",
    "\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_time: int,\n",
    "        c_in: int,\n",
    "        c_out: int,\n",
    "        downsample: bool = True,\n",
    "        n_layers: int = 1,\n",
    "        act: type[nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.c_time = c_time\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.downsample = downsample\n",
    "        self.n_layers = n_layers\n",
    "        self.act = act\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(ResBlock(c_time, c_in, c_out, stride=1, act=act))\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.convs.append(ResBlock(c_time, c_out, c_out, stride=1, act=act))\n",
    "        self.downsampler = nn.Conv2d(c_out, c_out, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(\n",
    "        self, x: InFeatureMapTensor, t: TimeStepEmbeddingTensor\n",
    "    ) -> OutFeatureMapTensor:\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, t)\n",
    "        if self.downsample:\n",
    "            x = self.downsampler(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 8, 8]), torch.Size([16, 2, 4, 4]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Downblock(32, 3, 2)\n",
    "with torch.no_grad():\n",
    "    yb = d(xb, tse)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class Upblock(nn.Module):\n",
    "    \"\"\"A superblock consisting of many upblocks of similar resolutions\n",
    "    and logic to use the activations of the counterpart downblock.\"\"\"\n",
    "\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        c_time: int,\n",
    "        c_in: int,\n",
    "        c_out: int,\n",
    "        upsample: bool = True,\n",
    "        n_layers: int = 1,\n",
    "        act: type[nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.c_time = c_time\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.upsample = upsample\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.upsampler = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(c_in, c_in, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.convs.append(ResBlock(c_time, c_in * 2, c_in, stride=1))\n",
    "        self.convs.append(ResBlock(c_time, c_in * 2, c_out, stride=1))\n",
    "\n",
    "    @classmethod\n",
    "    def from_downblock(cls, downblock):\n",
    "        return cls(\n",
    "            c_time=downblock.c_time,\n",
    "            c_in=downblock.c_out,\n",
    "            c_out=downblock.c_in,\n",
    "            upsample=downblock.downsample,\n",
    "            n_layers=downblock.n_layers,\n",
    "            act=downblock.act,\n",
    "        )\n",
    "\n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(\n",
    "        self, x: InFeatureMapTensor, downblock: Downblock, t: TimeStepEmbeddingTensor\n",
    "    ) -> OutFeatureMapTensor:\n",
    "        if self.upsample:\n",
    "            x = self.upsampler(x)\n",
    "        for up, down in zip(self.convs, reversed(downblock.convs)):\n",
    "            x = up(torch.cat((x, down.output), dim=1), t)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = Upblock.from_downblock(d)\n",
    "with torch.no_grad():\n",
    "    xp = u(yb, d, tse)\n",
    "xb.shape == xp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class TimeEmbeddingMLP(nn.Module):\n",
    "    \"\"\"Small neural network to modify the \"raw\" time embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, c_in: int, c_out: int, act=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.c_out = c_out\n",
    "        self.time_emb_mlp = nn.Sequential(\n",
    "            nn.BatchNorm1d(c_in),\n",
    "            nn.Linear(c_in, c_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(c_out, c_out),\n",
    "        )\n",
    "\n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(self, t: TimeStepTensor) -> TimeStepEmbeddingTensor:\n",
    "        # Look up the sin/cos embedding  of the time step\n",
    "        x = timestep_embedding(t, self.c_in).to(t.device)\n",
    "        # Allow the model to slightly modify the embeddings\n",
    "        x = self.time_emb_mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"Diffusion U-net with a diffusion time dimension\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nfs: Sequence[int],\n",
    "        n_blocks: Sequence[int],\n",
    "        color_channels: int = 3,\n",
    "        act: type[nn.Module] = nn.ReLU,\n",
    "    ):\n",
    "        assert len(n_blocks) - 1 == len(nfs)\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_embedding = TimeEmbeddingMLP(nfs[0], 4 * nfs[0])\n",
    "        c_time = self.time_embedding.c_out\n",
    "\n",
    "        # Since we use pre-activation ResBlocks, we need to use a Conv2d here\n",
    "        # to avoid discarding pixel information\n",
    "        self.start = nn.Conv2d(\n",
    "            color_channels, nfs[0], kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
    "        )\n",
    "        self.downblocks = nn.ModuleList()\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for c_in, c_out, n_layers in zip(nfs, nfs[1:], n_blocks):\n",
    "            db = Downblock(\n",
    "                c_time,\n",
    "                c_in,\n",
    "                c_out,\n",
    "                n_layers=n_layers,\n",
    "                act=act,\n",
    "            )\n",
    "            self.downblocks.append(db)\n",
    "            self.upblocks.insert(0, Upblock.from_downblock(db))\n",
    "        self.middle = ResBlock(c_time, nfs[-1], nfs[-1], stride=1, act=act)\n",
    "        self.end = ConvBlock(nfs[0], color_channels, act=act)\n",
    "\n",
    "    # Uniquely for a U-net module output dimensions must match the input dimensions\n",
    "    @jaxtyped(typechecker=beartype)\n",
    "    def forward(self, x_t: InFeatureMapTensor, t: TimeStepTensor) -> InFeatureMapTensor:\n",
    "        te = self.time_embedding(t)\n",
    "        _, c, _, _ = x_t.shape\n",
    "        if c != self.start.in_channels:\n",
    "            raise ValueError(\"model color channels must match input data channels\")\n",
    "        x = self.start(x_t)\n",
    "        for db in self.downblocks:\n",
    "            x = db(x, te)\n",
    "        x = self.middle(x, te)\n",
    "        for ub, db in zip(self.upblocks, reversed(self.downblocks)):\n",
    "            x = ub(x, db, te)\n",
    "        return self.end(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we can do a forward prop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet(\n",
    "    nfs=(224, 448, 672, 896),\n",
    "    n_blocks=(3, 2, 2, 1, 1),\n",
    "    color_channels=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 8, 8]), torch.Size([16]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb = torch.linspace(0, 1, 16)\n",
    "xb.shape, tb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yb = unet(xb, tb)\n",
    "\n",
    "assert xb.shape == yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use this size to quickly test the pipeline in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |exports\n",
    "def get_tiny_unet():\n",
    "    return UnetLightning(\n",
    "        nfs=(224, 448, 672, 896),\n",
    "        n_blocks=(3, 2, 2, 1, 1),\n",
    "        color_channels=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SlowAI",
   "language": "python",
   "name": "slowai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
