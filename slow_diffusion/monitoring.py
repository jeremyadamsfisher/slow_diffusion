# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_monitoring.ipynb.

# %% auto 0
__all__ = ['MonitorCallback', 'CountDeadUnitsCallback', 'Stats', 'StatsCallback']

# %% ../nbs/05_monitoring.ipynb 2
import re
from argparse import Namespace
from collections import Counter

import lightning as L
import matplotlib.pyplot as plt
from glom import glom
from lightning.pytorch.loggers import WandbLogger
from torch import nn

import wandb
from .fashionmnist import FashionMNISTDataModule
from .training import get_tiny_unet_lightning

# %% ../nbs/05_monitoring.ipynb 5
class MonitorCallback(L.Callback):
    """Log arbitrary properties in the training run, such as LR."""

    def __init__(self, gloms: dict[str, str]):
        super().__init__()
        self.gloms = gloms

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        args = Namespace(
            trainer=trainer,
            pl_module=pl_module,
            outputs=outputs,
            batch=batch,
            batch_idx=batch_idx,
        )
        for name, spec in self.gloms.items():
            self.log(name, glom(args, spec), on_step=True)

# %% ../nbs/05_monitoring.ipynb 9
class CountDeadUnitsCallback(L.Callback):
    """Check for numeric underflow or overflow"""

    def __init__(self):
        super().__init__()

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        nans = 0
        zeros = 0
        for _, params in pl_module.named_parameters():
            nans += params.isnan().int().sum().item()
            zeros += (params == 0).sum().item()
        self.log("nans", nans, reduce_fx=max)
        self.log("zeros", zeros, reduce_fx=max)

# %% ../nbs/05_monitoring.ipynb 12
class Stats:
    def __init__(self, label, module, log, live):
        self.label = label
        self.hook = module.register_forward_hook(self.append)
        self.log = log
        self.live = live
        self.means = []
        self.stds = []

    def append(self, module, _, activations):
        if not module.training:
            return
        activations = activations.cpu()
        mean = activations.mean().cpu().item()
        std = activations.std().cpu().item()
        if self.live:
            self.log(f"{self.label}:mean", mean)
            self.log(f"{self.label}:std", std)
        else:
            self.means.append(mean)
            self.stds.append(std)

    def plot(self, ax0, ax1):
        ax0.plot(self.means)
        ax1.plot(self.stds, label=self.label)

    def cleanup(self):
        self.hook.remove()


class StatsCallback(L.Callback):
    def __init__(
        self,
        mods: list[type[nn.Module]] | None = None,
        mod_filter: str | None = None,
        live=False,
    ):
        assert mods or mod_filter
        self.mods = []
        if mods is not None:
            self.mods.extend(mods)
        self.mod_filter = mod_filter
        self.mod_stats = []
        self.live = live

    def on_fit_start(self, trainer, pl_module):
        c = Counter()
        for mod in self.mods:
            cls_name = mod.__class__.__name__
            name = f"{cls_name}:{c.get(cls_name)}"
            s = Stats(name, mod, self.log, self.live)
            self.mod_stats.append(s)
            c.update((cls_name,))

        if self.mod_filter is not None:
            for name, mod in pl_module.named_modules():
                if re.match(self.mod_filter, name):
                    s = Stats(name, mod, self.log, self.live)
                    self.mod_stats.append(s)

    def plot(self, log=True):
        with plt.style.context("ggplot"):
            fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(8, 3))
            ax0.set(title="Means", xlabel="Time Step", ylabel="Activation")
            ax1.set(title="STDs", xlabel="Time Step")
            for mod_stat in self.mod_stats:
                mod_stat.plot(ax0, ax1)
            fig.legend(loc=7)
            fig.subplots_adjust(right=0.75)
            return fig

    def log(self):
        if not self.live:
            fig = self.plot()
            img = wandb.Image(fig)
            wandb.log({"stats": img})

    def on_train_epoch_end(self, trainer, pl_module):
        self.log()

    def cleanup(self):
        for s in self.mod_stats:
            s.cleanup()

    def on_fit_end(self, trainer, pl_module):
        self.log()
        self.cleanup()

    def on_exception(self, trainer, pl_module, exception):
        self.log()
        self.cleanup()
