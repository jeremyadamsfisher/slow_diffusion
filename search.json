[
  {
    "objectID": "initialization.html",
    "href": "initialization.html",
    "title": "Initialization",
    "section": "",
    "text": "dm = FashionMNISTDataModule(bs=128)\ndm.setup()\n\n\n((x_t, t), ε) = next(iter(dm.train_dataloader()))\nx_t.shape\n\ntorch.Size([128, 1, 32, 32])\n\n\nLet’s verify that the distribution remains normal after the transformation is applied.\n\n@singledispatch\ndef kaiming(module):\n    ...\n\n\n@kaiming.register(PreactConvBlock)\ndef _(c):\n    if isinstance(c.act, nn.ReLU):\n        torch.nn.init.kaiming_normal_(c.conv.weight, a=0.0)\n    elif isinstance(c.act, nn.SiLU):\n        torch.nn.init.kaiming_normal_(c.conv.weight, a=0.1)\n    else:\n        raise ValueError\n    if c.conv.bias is not None:\n        torch.nn.init.constant_(c.conv.bias, 0)\n\n\nconv_relu_default = PreactConvBlock(1, 1, act=nn.ReLU)\nconv_relu = PreactConvBlock(1, 1, act=nn.ReLU)\nconv_silu_default = PreactConvBlock(1, 1, act=nn.SiLU)\nconv_silu = PreactConvBlock(1, 1, act=nn.SiLU)\nfor m in [conv_relu, conv_silu]:\n    kaiming(m)\n\n\ndef plot(xb, args, modules: list[tuple[str, nn.Module]]):\n    fig, axes = plt.subplots(1, len(modules), figsize=(4 * len(modules), 4))\n    for ax, (label, c) in zip(axes, cs):\n        _, bins, _ = ax.hist(xb.reshape(-1), bins=30, alpha=0.5, label=\"input\")\n        with torch.no_grad():\n            yb = c(*args)\n        ax.hist(yb.reshape(-1), bins, alpha=0.33, label=label)\n        ax.set(xlabel=\"Logit magnitude\", ylabel=\"Frequency\", title=label)\n    fig.tight_layout()\n\n\ncs = [\n    (\"default relu\", conv_relu_default),\n    (\"kaiming relu\", conv_relu),\n    (\"default silu\", conv_silu_default),\n    (\"kaiming silu\", conv_silu),\n]\nplot(x_t, (x_t,), cs)\n\n\n\n\n\n\n\n\nGood! The Kaiming methods preserve the distribution variance, unlike the default.\nLet’s start with initializing with Kaiming and see how we might need to adjust the ResBlocks.\n\ndef append(module, _, acts):\n    module.acts = acts\n\n\n@contextmanager\ndef monitor_activations(model):\n    hooks = []\n    for name, m in model.unet.named_modules():\n        hooks.append(m.register_forward_hook(append))\n    yield\n    for hook in hooks:\n        hook.remove()\n\n\nmodel = get_tiny_unet_lightning(act=nn.ReLU)\nmodel = model.apply(kaiming)\n\nwith monitor_activations(model), torch.no_grad():\n    model.step(((x_t, t), ε))\n\n\ncolor_map = {\n    \"Downblock\": \"b\",\n    \"Upblock\": \"r\",\n    \"Conv2d\": \"g\",\n    \"ConvBlock\": \"purple\",\n    \"Input\": \"orange\",\n    \"PreactResBlock\": \"black\",\n    \"NonPreactResBlock\": \"black\",\n}\n\n\ndef plot_per_layer_stats(mods, name=None, axes=None):\n    if axes is None:\n        fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 5))\n    else:\n        ax0, ax1 = axes\n    if name:\n        ax0.set(title=f\"{name} means\")\n        ax1.set(title=f\"{name} STDs\")\n    else:\n        ax0.set(title=\"Means\")\n        ax1.set(title=\"STDs\")\n\n    data = []\n    data.append((x_t.mean(), x_t.std(), \"Input\"))\n    for m in mods:\n        clsname = m.__class__.__name__\n        row = (m.acts.mean(), m.acts.std(), clsname)\n        data.append(row)\n    means, stds, clss = zip(*data)\n    idxs = torch.arange(0, len(data))\n    colors = [color_map[c] for c in clss]\n    ax0.scatter(idxs, means, c=colors)\n    ax1.scatter(idxs, stds, c=colors)\n\n\ndef plot_per_resnet_stats(m):\n    resnets = []\n    for block in [*m.unet.downblocks, *m.unet.upblocks]:\n        for conv in block.convs:\n            resnets.append(conv)\n    plot_per_layer_stats(resnets)\n\n\nplot_per_resnet_stats(model)\n\n\n\n\n\n\n\n\nNothing suspicious after one iteration. Let’s train it and see what happens.\n\ndef plot_per_layer_stats_after_training(\n    model, max_steps=10, name=None, axes=None, **kwargs\n):\n    trainer = L.Trainer(\n        max_epochs=1,\n        precision=\"bf16-mixed\",\n        max_steps=max_steps,\n        enable_progress_bar=False,\n        enable_model_summary=False,\n        **kwargs,\n    )\n    trainer.fit(model=model, datamodule=dm)\n    with monitor_activations(model), torch.no_grad():\n        model.step(((x_t, t), ε))\n    resnets = []\n    for block in [*model.unet.downblocks, *model.unet.upblocks]:\n        for conv in block.convs:\n            resnets.append(conv)\n    plot_per_layer_stats(resnets, name, axes)\n    return model\n\n\nmax_steps = 50\nexperiments = [\n    # (nn.ReLU, True, \"PreactResBlock\"),\n    # (nn.ReLU, False, \"PreactResBlock\"),\n    (nn.SiLU, True, \"NonPreactResBlock\"),\n    (nn.SiLU, False, \"NonPreactResBlock\"),\n    (nn.SiLU, True, \"PreactResBlock\"),\n    (nn.SiLU, False, \"PreactResBlock\"),\n]\nn = len(experiments)\nk = 7\nfig, axes = plt.subplots(n, 2, figsize=(2 * k, 0.5 * n * k))\nfor i, (act, kaiming_, res_block_cls) in enumerate(experiments):\n    name = f\"{act.__name__};{res_block_cls}\"\n    if kaiming_:\n        name += \"+kaiming\"\n    model = get_tiny_unet_lightning(act=act, res_block_cls=res_block_cls)\n    if kaiming_:\n        model.apply(kaiming)\n    _ = plot_per_layer_stats_after_training(\n        model, max_steps=max_steps, name=name, axes=axes[i, :]\n    )\nfig.tight_layout()\n\nUsing bfloat16 Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoading `train_dataloader` to estimate number of stepping batches.\n`Trainer.fit` stopped: `max_steps=50` reached.\nUsing bfloat16 Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoading `train_dataloader` to estimate number of stepping batches.\n`Trainer.fit` stopped: `max_steps=50` reached.\nUsing bfloat16 Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoading `train_dataloader` to estimate number of stepping batches.\n`Trainer.fit` stopped: `max_steps=50` reached.\nUsing bfloat16 Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoading `train_dataloader` to estimate number of stepping batches.\n`Trainer.fit` stopped: `max_steps=50` reached.\n\n\n\n\n\n\n\n\n\nNice! Looks like there’s some issue with the pre-activation block. Let’s use the classical configuration from now on.",
    "crumbs": [
      "Initialization"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Slow Diffusion",
    "section": "",
    "text": "I’m intrigued by Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget (Twitter thread) and will attempt to scale up my FastAI project using their recommendations.",
    "crumbs": [
      "Slow Diffusion"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Slow Diffusion",
    "section": "Install",
    "text": "Install\npip install slow_diffusion",
    "crumbs": [
      "Slow Diffusion"
    ]
  },
  {
    "objectID": "ddpm.html",
    "href": "ddpm.html",
    "title": "DDPM",
    "section": "",
    "text": "source\n\ndenoisify\n\n denoisify (x_t, noise, t)\n\n\nsource\n\n\nddpm\n\n ddpm (model, sz, n_steps, device=None)\n\n\nout = ddpm(get_tiny_unet_lightning().unet, (4, 1, 32, 32), 2)\nshow_images(out);\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  7.27time step/s]\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nDDPMCallback\n\n DDPMCallback (n_imgs=4, n_steps=100)\n\n*Abstract base class used to build new callbacks.\nSubclass this class and override any of the relevant hooks*\n\nwith wandb.init():\n    dm = TinyFashionMNISTDataModule(32, n_workers=0)\n    dm.setup()\n    wandb_logger = WandbLogger()\n    trainer = L.Trainer(\n        max_epochs=1, callbacks=[DDPMCallback(n_steps=2)], logger=wandb_logger\n    )\n    trainer.fit(model=get_tiny_unet_lightning(), datamodule=dm)\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: jfisher40. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.17.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.17.5\n\n\nRun data is saved locally in /home/jeremy/Desktop/slow_diffusion/nbs/wandb/run-20240831_110850-9cc9db9v\n\n\nSyncing run cerulean-smoke-74 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/jfisher40/slow_diffusion-nbs\n\n\n View run at https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/9cc9db9v\n\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nYou are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n/home/jeremy/micromamba/envs/slowai/lib/python3.11/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nLoading `train_dataloader` to estimate number of stepping batches.\n/home/jeremy/micromamba/envs/slowai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n/home/jeremy/micromamba/envs/slowai/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n\n  | Name    | Type    | Params | Mode \n--------------------------------------------\n0 | unet    | Unet    | 15.7 M | train\n1 | loss_fn | MSELoss | 0      | train\n--------------------------------------------\n15.7 M    Trainable params\n0         Non-trainable params\n15.7 M    Total params\n62.964    Total estimated model params size (MB)\n\n\n\n\n\n/home/jeremy/micromamba/envs/slowai/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n\n\n\n\n\n\n\n\n  0%|                                                                                                                                                  | 0/1 [00:00&lt;?, ?time step/s]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00,  4.15time step/s]\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\n\n\n\n\n\nRun history:\n\n\n\nepoch\n▁\n\n\ntest_loss\n▁\n\n\ntrainer/global_step\n▁\n\n\n\nRun summary:\n\n\n\nepoch\n0\n\n\ntest_loss\n0.824\n\n\ntrainer/global_step\n1\n\n\n\n\n\n\n View run cerulean-smoke-74 at: https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/9cc9db9v View project at: https://wandb.ai/jfisher40/slow_diffusion-nbsSynced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20240831_110850-9cc9db9v/logs\n\n\nThe new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information.",
    "crumbs": [
      "DDPM"
    ]
  },
  {
    "objectID": "fashion_mnist.html",
    "href": "fashion_mnist.html",
    "title": "Data - FashionMNIST",
    "section": "",
    "text": "source\n\nFashionMNISTDataModule\n\n FashionMNISTDataModule (bs, n_workers=0)\n\nFasion MNIST datamodule\n\n\nExported source\nclass FashionMNISTDataModule(DiffusionDataModule):\n    \"\"\"Fasion MNIST datamodule\"\"\"\n\n    def __init__(self, bs, n_workers=0):\n        super().__init__(\n            \"fashion_mnist\",\n            bs,\n            n_workers,\n            img_size=(32, 32),\n        )\n\n    def noisify_fn(self, x_0):\n        x_0 = F.convert_image_dtype(x_0, torch.float)\n        # zero-center so that the mean does not change after adding noise\n        x_0 -= 0.5\n        return noisify(x_0)\n\n\n\ndm = FashionMNISTDataModule(4)\ndm.setup()\n\n\ndef preview(dataloder, n=4):\n    (x_t, ts), _ = next(iter(dataloder))\n    show_images(x_t[:n], [f\"t={t.item():.2f}\" for t in ts[:n]])\n\n\npreview(dm.train_dataloader());\n\n\npreview(dm.val_dataloader());\n\n\nunet = UnetLightning(\n    nfs=(224, 448, 672, 896),\n    n_blocks=(3, 2, 2, 1, 1),\n    color_channels=1,\n)\ntrainer = L.Trainer(max_epochs=2, fast_dev_run=True)\ntrainer.fit(model=unet, datamodule=dm)\n\nFor debugging, only\n\nsource\n\n\nTinyFashionMNISTDataModule\n\n TinyFashionMNISTDataModule (bs, n_workers=0)\n\nFasion MNIST datamodule\n\n\nExported source\nclass TinyFashionMNISTDataModule(FashionMNISTDataModule):\n    def post_process(self, ds):\n        return ds[\"train\"].select(range(100)).train_test_split(test_size=0.5)",
    "crumbs": [
      "Data - FashionMNIST"
    ]
  },
  {
    "objectID": "monitoring.html",
    "href": "monitoring.html",
    "title": "Monitoring",
    "section": "",
    "text": "We want to make sure the model can be inspected\n\ndef test_run(callback):\n    with wandb.init():\n        dm = FashionMNISTDataModule(256, n_workers=0)\n        dm.setup()\n        model = get_tiny_unet_lightning()\n        trainer = L.Trainer(\n            max_epochs=1,\n            callbacks=[callback],\n            logger=WandbLogger(),\n            precision=\"bf16-mixed\",\n            log_every_n_steps=1,\n        )\n        trainer.fit(model=model, datamodule=dm)\n\n\nsource\n\nMonitorCallback\n\n MonitorCallback (gloms:dict[str,str])\n\nLog arbitrary properties in the training run, such as LR.\n\ntest_run(MonitorCallback({\"lr\": \"trainer.optimizers.0.param_groups.0.lr\"}))\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: jfisher40. Use `wandb login --relogin` to force relogin\n\n\nTracking run with wandb version 0.17.7\n\n\nRun data is saved locally in /Users/jeremiahfisher/Code/slow_diffusion/nbs/wandb/run-20240825_112257-86jhcqof\n\n\nSyncing run jolly-pine-70 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/jfisher40/slow_diffusion-nbs\n\n\n View run at https://wandb.ai/jfisher40/slow_diffusion-nbs/runs/86jhcqof\n\n\nUsing bfloat16 Automatic Mixed Precision (AMP)\nGPU available: True (mps), used: False\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\nLoading `train_dataloader` to estimate number of stepping batches.\n/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n\n  | Name    | Type    | Params | Mode \n--------------------------------------------\n0 | unet    | Unet    | 15.7 M | train\n1 | loss_fn | MSELoss | 0      | train\n--------------------------------------------\n15.7 M    Trainable params\n0         Non-trainable params\n15.7 M    Total params\n62.964    Total estimated model params size (MB)\n\n\nSanity Checking: |                                                                      | 0/? [00:00&lt;?, ?it/s]\n\n\n/Users/jeremiahfisher/miniforge3/envs/slow_diffusion/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n\n\nSanity Checking DataLoader 0:   0%|                                                     | 0/2 [00:00&lt;?, ?it/s]\n\n\n\nsource\n\n\nCountDeadUnitsCallback\n\n CountDeadUnitsCallback ()\n\nCheck for numeric underflow or overflow\n\n\nExported source\nclass CountDeadUnitsCallback(L.Callback):\n    \"\"\"Check for numeric underflow or overflow\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        nans = 0\n        zeros = 0\n        for _, params in pl_module.named_parameters():\n            nans += params.isnan().int().sum().item()\n            zeros += (params == 0).sum().item()\n        self.log(\"nans\", nans, reduce_fx=max)\n        self.log(\"zeros\", zeros, reduce_fx=max)\n\n\n\ntest_run(CountDeadUnitsCallback())\n\nCheck activation distribution metrics.\n\nsource\n\n\nStatsCallback\n\n StatsCallback (mods:list[type[torch.nn.modules.module.Module]]|None=None,\n                mod_filter:str|None=None, live=False)\n\n*Abstract base class used to build new callbacks.\nSubclass this class and override any of the relevant hooks*\n\n\nExported source\nclass Stats:\n    def __init__(self, label, module, log, live):\n        self.label = label\n        self.hook = module.register_forward_hook(self.append)\n        self.log = log\n        self.live = live\n        self.means = []\n        self.stds = []\n\n    def append(self, module, _, activations):\n        if not module.training:\n            return\n        activations = activations.cpu()\n        mean = activations.mean().cpu().item()\n        std = activations.std().cpu().item()\n        if self.live:\n            self.log(f\"{self.label}:mean\", mean)\n            self.log(f\"{self.label}:std\", std)\n        else:\n            self.means.append(mean)\n            self.stds.append(std)\n\n    def plot(self, ax0, ax1):\n        ax0.plot(self.means)\n        ax1.plot(self.stds, label=self.label)\n\n    def cleanup(self):\n        self.hook.remove()\n\n\nclass StatsCallback(L.Callback):\n    def __init__(\n        self,\n        mods: list[type[nn.Module]] | None = None,\n        mod_filter: str | None = None,\n        live=False,\n    ):\n        assert mods or mod_filter\n        self.mods = []\n        if mods is not None:\n            self.mods.extend(mods)\n        self.mod_filter = mod_filter\n        self.mod_stats = []\n        self.live = live\n\n    def on_fit_start(self, trainer, pl_module):\n        c = Counter()\n        for mod in self.mods:\n            cls_name = mod.__class__.__name__\n            name = f\"{cls_name}:{c.get(cls_name)}\"\n            s = Stats(name, mod, self.log, self.live)\n            self.mod_stats.append(s)\n            c.update((cls_name,))\n\n        if self.mod_filter is not None:\n            for name, mod in pl_module.named_modules():\n                if re.match(self.mod_filter, name):\n                    s = Stats(name, mod, self.log, self.live)\n                    self.mod_stats.append(s)\n\n    def plot(self, log=True):\n        with plt.style.context(\"ggplot\"):\n            fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(8, 3))\n            ax0.set(title=\"Means\", xlabel=\"Time Step\", ylabel=\"Activation\")\n            ax1.set(title=\"STDs\", xlabel=\"Time Step\")\n            for mod_stat in self.mod_stats:\n                mod_stat.plot(ax0, ax1)\n            fig.legend(loc=7)\n            fig.subplots_adjust(right=0.75)\n            return fig\n\n    def log_stats(self):\n        if not self.live:\n            fig = self.plot()\n            img = wandb.Image(fig)\n            wandb.log({\"stats\": img})\n            plt.close(fig)\n\n    def on_train_epoch_end(self, trainer, pl_module):\n        self.log_stats()\n\n    def cleanup(self):\n        for s in self.mod_stats:\n            s.cleanup()\n\n    def on_fit_end(self, trainer, pl_module):\n        self.log_stats()\n        self.cleanup()\n\n    def on_exception(self, trainer, pl_module, exception):\n        self.log_stats()\n        self.cleanup()\n\n\n\nsource\n\n\nStats\n\n Stats (label, module, log, live)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncb = StatsCallback(mod_filter=r\"unet.(((down|up)blocks.\\d+)|start|middle|end)(?!\\.)\")\ntest_run(cb)\n\nYou can see how bad the training dynamics are initially",
    "crumbs": [
      "Monitoring"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "This is adapted from my FastAI notes\nExported source\nInFeatureMapTensor: TypeAlias = Float[Tensor, \"bs c_in h_in w_in\"]\nOutFeatureMapTensor: TypeAlias = Float[Tensor, \"bs c_out h_out w_out\"]\nTimeStepTensor: TypeAlias = Float[Tensor, \"bs\"]  # from 0 to 1\nTimeStepEmbeddingTensor: TypeAlias = Float[Tensor, \"bs t\"]",
    "crumbs": [
      "Model"
    ]
  },
  {
    "objectID": "model.html#modules",
    "href": "model.html#modules",
    "title": "Model",
    "section": "Modules",
    "text": "Modules\nNow, let’s get to the module definitions.\nThe ConvBlock is laid out in the “Preactivation” configuration, like so:\n\nThis does mean that we need to take care in the first block to use a “raw” nn.Conv, because otherwise the activation would discard pixel information.\n\nsource\n\nPreactConvBlock\n\n PreactConvBlock (c_in:int, c_out:int,\n                  act:type[torch.nn.modules.module.Module], ks:int=3,\n                  stride:int=1)\n\nWrapper for a Conv block with normalization and activation\n\n\nExported source\nclass PreactConvBlock(nn.Module):\n    \"\"\"Wrapper for a Conv block with normalization and activation\"\"\"\n\n    @beartype\n    def __init__(\n        self,\n        c_in: int,\n        c_out: int,\n        act: type[nn.Module],\n        ks: int = 3,\n        stride: int = 1,\n    ):\n        super().__init__()\n        self.ks = ks\n        self.norm = nn.BatchNorm2d(c_in)\n        self.act = act()\n        self.conv = nn.Conv2d(\n            c_in,\n            c_out,\n            stride=stride,\n            kernel_size=ks,\n            padding=ks // 2,\n            bias=False,\n        )\n\n    @jaxtyped(typechecker=beartype)\n    def forward(self, x: InFeatureMapTensor) -&gt; OutFeatureMapTensor:\n        x = self.norm(x)\n        x = self.act(x)\n        x = self.conv(x)\n        return x\n\n\n\nsource\n\n\ntimestep_embedding\n\n timestep_embedding (ts:jaxtyping.Float[Tensor,'bs'], emb_dim:int,\n                     max_period:int=10000)\n\n\n\nExported source\n@jaxtyped(typechecker=beartype)\ndef timestep_embedding(\n    ts: TimeStepTensor, emb_dim: int, max_period: int = 10_000\n) -&gt; TimeStepEmbeddingTensor:\n    exponent = -math.log(max_period) * torch.linspace(\n        0, 1, emb_dim // 2, device=ts.device\n    )\n    embedding = ts[:, None].float() * exponent.exp()[None, :]\n    embedding = torch.cat([embedding.sin(), embedding.cos()], dim=-1)\n    return embedding\n\n\n\nsource\n\n\nTimeEmbeddingMixer\n\n TimeEmbeddingMixer (c_time:int, c_out:int,\n                     act:type[torch.nn.modules.module.Module]=&lt;class\n                     'torch.nn.modules.activation.ReLU'&gt;)\n\nIncorporate the time embedding into the ResBlock logits\n\n\nExported source\nclass TimeEmbeddingMixer(nn.Module):\n    \"\"\"Incorporate the time embedding into the ResBlock logits\"\"\"\n\n    @beartype\n    def __init__(self, c_time: int, c_out: int, act: type[nn.Module] = nn.ReLU):\n        super().__init__()\n        self.lin = nn.Linear(c_time, c_out * 2)\n        self.act = act()\n\n    @jaxtyped(typechecker=beartype)\n    def forward(\n        self, x: InFeatureMapTensor, t_emb: TimeStepEmbeddingTensor\n    ) -&gt; OutFeatureMapTensor:\n        t_emb = self.lin(self.act(t_emb))[:, :, None, None]\n        scale, shift = torch.chunk(t_emb, 2, dim=1)\n        return x * (1 + scale) + shift\n\n\n\nsource\n\n\nPreactResBlock\n\n PreactResBlock (c_time:int, c_in:int, c_out:int,\n                 act:type[torch.nn.modules.module.Module])\n\nConv resblock with the preactivation configuration and time embedding modulation\n\n\nExported source\nclass PreactResBlock(nn.Module):\n    \"\"\"Conv resblock with the preactivation configuration and time embedding modulation\"\"\"\n\n    @beartype\n    def __init__(\n        self,\n        c_time: int,\n        c_in: int,\n        c_out: int,\n        act: type[nn.Module],\n    ):\n        super().__init__()\n        self.c_time = c_time\n        self.c_in = c_in\n        self.c_out = c_out\n\n        self.time_mixer = TimeEmbeddingMixer(c_time, c_out, act=act)\n        self.conv_a = PreactConvBlock(c_in, c_out, act=act)\n        self.conv_b = PreactConvBlock(c_out, c_out, act=act)\n        if c_in != c_out:\n            self.id_conv = nn.Conv2d(c_in, c_out, kernel_size=1)\n        else:\n            self.id_conv = None\n\n        # Used for the Unet cross-link\n        self.output = None\n\n    def non_residual(self, x, t_emb):\n        x = self.conv_a(x)\n        x = self.time_mixer(x, t_emb)\n        x = self.conv_b(x)\n        return x\n\n    def residual(self, x):\n        if self.id_conv is not None:\n            return self.id_conv(x)\n        else:\n            return x\n\n    @jaxtyped(typechecker=beartype)\n    def forward(\n        self, x: InFeatureMapTensor, t_emb: TimeStepEmbeddingTensor\n    ) -&gt; OutFeatureMapTensor:\n        x = self.non_residual(x, t_emb) + self.residual(x)\n        self.output = x\n        return x\n\n\n\nsource\n\n\nNonPreactResBlock\n\n NonPreactResBlock (c_time:int, c_in:int, c_out:int,\n                    act:type[torch.nn.modules.module.Module])\n\nConv resblock with the classic residual configuration and time embedding modulation\n\n\nExported source\nclass NonPreactResBlock(nn.Module):\n    \"\"\"Conv resblock with the classic residual configuration and time embedding modulation\"\"\"\n\n    @beartype\n    def __init__(\n        self,\n        c_time: int,\n        c_in: int,\n        c_out: int,\n        act: type[nn.Module],\n    ):\n        super().__init__()\n        self.c_time = c_time\n        self.c_in = c_in\n        self.c_out = c_out\n\n        self.time_mixer = TimeEmbeddingMixer(c_time, c_out, act=act)\n        self.conv_a = nn.Conv2d(c_in, c_out, kernel_size=3, padding=3 // 2, bias=False)\n        self.norm_a = nn.BatchNorm2d(c_out)\n        self.act_a = act()\n        self.conv_b = nn.Conv2d(c_out, c_out, kernel_size=3, padding=3 // 2, bias=False)\n        self.norm_b = nn.BatchNorm2d(c_out)\n        self.act_b = act()\n\n        if c_in != c_out:\n            self.id_conv = nn.Conv2d(c_in, c_out, kernel_size=1)\n        else:\n            self.id_conv = None\n\n        # Used for the Unet cross-link\n        self.output = None\n\n    @jaxtyped(typechecker=beartype)\n    def forward(\n        self, x_orig: InFeatureMapTensor, t_emb: TimeStepEmbeddingTensor\n    ) -&gt; OutFeatureMapTensor:\n        x = self.conv_a(x_orig)\n        x = self.norm_a(x)\n        x = self.act_a(x)\n        x = self.conv_b(x)\n        x = self.norm_b(x)\n        x = self.time_mixer(x, t_emb)\n        if self.id_conv:\n            x_orig = self.id_conv(x_orig)\n        x += x_orig\n        x = self.act_b(x)\n        self.output = x\n        return x\n\n\n\nxb = torch.randn(16, 3, 8, 8)\n\n\nbs = xb.shape[0]\nts: TimeStepTensor = torch.linspace(-10, 10, bs)\ntse = timestep_embedding(ts, 32)\ntse.shape\n\ntorch.Size([16, 32])\n\n\n\nts\n\ntensor([-10.0000,  -8.6667,  -7.3333,  -6.0000,  -4.6667,  -3.3333,  -2.0000,\n         -0.6667,   0.6667,   2.0000,   3.3333,   4.6667,   6.0000,   7.3333,\n          8.6667,  10.0000])\n\n\n\nsource\n\n\nDownblock\n\n Downblock (c_time:int, c_in:int, c_out:int,\n            act:type[torch.nn.modules.module.Module],\n            downsample:bool=True, n_layers:int=1,\n            res_block_cls:type[torch.nn.modules.module.Module]=&lt;class\n            '__main__.PreactResBlock'&gt;)\n\nA superblock consisting of many downblocks of similar resolutions\n\n\nExported source\nclass Downblock(nn.Module):\n    \"\"\"A superblock consisting of many downblocks of similar resolutions\"\"\"\n\n    @beartype\n    def __init__(\n        self,\n        c_time: int,\n        c_in: int,\n        c_out: int,\n        act: type[nn.Module],\n        downsample: bool = True,\n        n_layers: int = 1,\n        res_block_cls: type[nn.Module] = PreactResBlock,\n    ):\n        super().__init__()\n        self.c_time = c_time\n        self.c_in = c_in\n        self.c_out = c_out\n        self.downsample = downsample\n        self.n_layers = n_layers\n        self.act = act\n        self.res_block_cls = res_block_cls\n\n        self.convs = nn.ModuleList()\n        self.convs.append(res_block_cls(c_time, c_in, c_out, act=act))\n        for _ in range(n_layers - 1):\n            self.convs.append(res_block_cls(c_time, c_out, c_out, act=act))\n        self.downsampler = nn.Conv2d(c_out, c_out, kernel_size=3, stride=2, padding=1)\n\n    @jaxtyped(typechecker=beartype)\n    def forward(\n        self, x: InFeatureMapTensor, t: TimeStepEmbeddingTensor\n    ) -&gt; OutFeatureMapTensor:\n        for conv in self.convs:\n            x = conv(x, t)\n        if self.downsample:\n            x = self.downsampler(x)\n        return x\n\n\n\nd = Downblock(32, 3, 2, act=nn.ReLU)\nwith torch.no_grad():\n    yb = d(xb, tse)\nxb.shape, yb.shape\n\n(torch.Size([16, 3, 8, 8]), torch.Size([16, 2, 4, 4]))\n\n\n\nsource\n\n\nUpblock\n\n Upblock (c_time:int, c_in:int, c_out:int,\n          act:type[torch.nn.modules.module.Module], upsample:bool=True,\n          n_layers:int=1,\n          res_block_cls:type[torch.nn.modules.module.Module]=&lt;class\n          '__main__.PreactResBlock'&gt;)\n\nA superblock consisting of many upblocks of similar resolutions and logic to use the activations of the counterpart downblock.\n\n\nExported source\nclass Upblock(nn.Module):\n    \"\"\"A superblock consisting of many upblocks of similar resolutions\n    and logic to use the activations of the counterpart downblock.\"\"\"\n\n    @beartype\n    def __init__(\n        self,\n        c_time: int,\n        c_in: int,\n        c_out: int,\n        act: type[nn.Module],\n        upsample: bool = True,\n        n_layers: int = 1,\n        res_block_cls: type[nn.Module] = PreactResBlock,\n    ):\n        super().__init__()\n        self.c_time = c_time\n        self.c_in = c_in\n        self.c_out = c_out\n        self.upsample = upsample\n        self.n_layers = n_layers\n\n        self.upsampler = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(c_in, c_in, kernel_size=3, padding=1),\n        )\n        self.convs = nn.ModuleList()\n        for _ in range(n_layers - 1):\n            self.convs.append(res_block_cls(c_time, c_in * 2, c_in, act=act))\n        self.convs.append(res_block_cls(c_time, c_in * 2, c_out, act=act))\n\n    @classmethod\n    def from_downblock(cls, downblock):\n        return cls(\n            c_time=downblock.c_time,\n            c_in=downblock.c_out,\n            c_out=downblock.c_in,\n            upsample=downblock.downsample,\n            n_layers=downblock.n_layers,\n            act=downblock.act,\n            res_block_cls=downblock.res_block_cls,\n        )\n\n    @jaxtyped(typechecker=beartype)\n    def forward(\n        self, x: InFeatureMapTensor, downblock: Downblock, t: TimeStepEmbeddingTensor\n    ) -&gt; OutFeatureMapTensor:\n        if self.upsample:\n            x = self.upsampler(x)\n        for up, down in zip(self.convs, reversed(downblock.convs)):\n            x = up(torch.cat((x, down.output), dim=1), t)\n        return x\n\n\n\nu = Upblock.from_downblock(d)\nwith torch.no_grad():\n    xp = u(yb, d, tse)\nxb.shape == xp.shape\n\nTrue\n\n\n\nsource\n\n\nTimeEmbeddingMLP\n\n TimeEmbeddingMLP (c_in:int, c_out:int, act=&lt;class\n                   'torch.nn.modules.activation.ReLU'&gt;)\n\nSmall neural network to modify the “raw” time embeddings\n\n\nExported source\nclass TimeEmbeddingMLP(nn.Module):\n    \"\"\"Small neural network to modify the \"raw\" time embeddings\"\"\"\n\n    def __init__(self, c_in: int, c_out: int, act=nn.ReLU):\n        super().__init__()\n        self.c_in = c_in\n        self.c_out = c_out\n        self.time_emb_mlp = nn.Sequential(\n            nn.BatchNorm1d(c_in),\n            nn.Linear(c_in, c_out),\n            nn.ReLU(),\n            nn.Linear(c_out, c_out),\n        )\n\n    @jaxtyped(typechecker=beartype)\n    def forward(self, t: TimeStepTensor) -&gt; TimeStepEmbeddingTensor:\n        # Look up the sin/cos embedding  of the time step\n        x = timestep_embedding(t, self.c_in).to(t.device)\n        # Allow the model to slightly modify the embeddings\n        x = self.time_emb_mlp(x)\n        return x\n\n\n\nsource\n\n\nUnet\n\n Unet (nfs:Sequence[int], n_blocks:Sequence[int],\n       act:type[torch.nn.modules.module.Module], color_channels:int=3,\n       res_block_cls:type[torch.nn.modules.module.Module]=&lt;class\n       '__main__.PreactResBlock'&gt;)\n\nDiffusion U-net with a diffusion time dimension\n\n\nExported source\nclass Unet(nn.Module):\n    \"\"\"Diffusion U-net with a diffusion time dimension\"\"\"\n\n    def __init__(\n        self,\n        nfs: Sequence[int],\n        n_blocks: Sequence[int],\n        act: type[nn.Module],\n        color_channels: int = 3,\n        res_block_cls: type[nn.Module] = PreactResBlock,\n    ):\n        assert len(n_blocks) - 1 == len(nfs)\n        super().__init__()\n\n        self.time_embedding = TimeEmbeddingMLP(nfs[0], 4 * nfs[0])\n        c_time = self.time_embedding.c_out\n\n        # Since we use pre-activation ResBlocks, we need to use a Conv2d here\n        # to avoid discarding pixel information\n        self.start = nn.Conv2d(\n            color_channels, nfs[0], kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        )\n        self.downblocks = nn.ModuleList()\n        self.upblocks = nn.ModuleList()\n        for c_in, c_out, n_layers in zip(nfs, nfs[1:], n_blocks):\n            db = Downblock(\n                c_time,\n                c_in,\n                c_out,\n                n_layers=n_layers,\n                act=act,\n                res_block_cls=res_block_cls,\n            )\n            self.downblocks.append(db)\n            self.upblocks.insert(0, Upblock.from_downblock(db))\n        self.middle = res_block_cls(c_time, nfs[-1], nfs[-1], act=act)\n\n        # vvv double check this\n        self.end = PreactConvBlock(nfs[0], color_channels, act=act)\n\n    # Uniquely for a U-net module output dimensions must match the input dimensions\n    @jaxtyped(typechecker=beartype)\n    def forward(self, x_t: InFeatureMapTensor, t: TimeStepTensor) -&gt; InFeatureMapTensor:\n        te = self.time_embedding(t)\n        _, c, _, _ = x_t.shape\n        if c != self.start.in_channels:\n            raise ValueError(\"model color channels must match input data channels\")\n        x = self.start(x_t)\n        for db in self.downblocks:\n            x = db(x, te)\n        x = self.middle(x, te)\n        for ub, db in zip(self.upblocks, reversed(self.downblocks)):\n            x = ub(x, db, te)\n        return self.end(x)\n\n\nLet’s make sure we can do a forward prop.\n\nunet = Unet(\n    nfs=(224, 448, 672, 896),\n    n_blocks=(3, 2, 2, 1, 1),\n    color_channels=3,\n    act=nn.ReLU,\n)\n\n\ntb = torch.linspace(0, 1, 16)\nxb.shape, tb.shape\n\n(torch.Size([16, 3, 8, 8]), torch.Size([16]))\n\n\n\nwith torch.no_grad():\n    yb = unet(xb, tb)\n\nassert xb.shape == yb.shape",
    "crumbs": [
      "Model"
    ]
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training Algorithm",
    "section": "",
    "text": "source\n\nUnetLightning\n\n UnetLightning (nfs:Sequence[int], n_blocks:Sequence[int],\n                color_channels:int, lr:float=0.004,\n                one_cycle_pct_start:float=0.3,\n                one_cycle_div_factor:float=25.0,\n                one_cycle_final_div_factor:float=10000.0,\n                adamw_epsilon:float=1e-05, act:type[torch.nn.modules.modul\n                e.Module]|str='torch.nn.ReLU', res_block_cls:type[torch.nn\n                .modules.module.Module]|str='PreactResBlock',\n                kaiming:bool=False)\n\nHooks to be used in LightningModule.\n\n\nExported source\nclass UnetLightning(L.LightningModule):\n    @beartype\n    def __init__(\n        self,\n        nfs: Sequence[int],\n        n_blocks: Sequence[int],\n        color_channels: int,\n        lr: float = 4e-3,\n        one_cycle_pct_start: float = 0.3,\n        one_cycle_div_factor: float = 25.0,\n        one_cycle_final_div_factor: float = 1e4,\n        adamw_epsilon: float = 1e-5,\n        act: type[torch.nn.Module] | str = \"torch.nn.ReLU\",\n        res_block_cls: type[nn.Module] | str = \"PreactResBlock\",\n        kaiming: bool = False,\n    ):\n        \"\"\"Unet training code\n\n        Args:\n            nfs:  Number of channels in a {Up,Down}block\n            n_blocks: Number of sub-blocks in {Up,Down}block. Should have 1\n                more entry than `nfs`\n            color_channels: Color channels, or however many dimensions in the\n                VAE bottleneck space\n            lr: learning rate\n            one_cycle_pct_start: The percentage of the cycle (in number of\n                steps) spent increasing the learning rate. Default: 0.3\n            one_cycle_div_factor:  Determines the initial learning rate\n                via initial_lr = max_lr/div_factor Default: 25\n            one_cycle_final_div_factor: Determines the minimum learning rate\n                via min_lr = initial_lr/final_div_factor Default: 1e4\n            adamw_epsilon: term added to the denominator to improve numerical\n                stability. PyTorch defaults to 1e-8. 1e-5 is better.\n            act: activation function\n            res_block_cls: classic or preactivation resblock\n            kaiming: perform kaiming initialization\n        \"\"\"\n        super().__init__()\n        if isinstance(act, str):\n            act = eval(act)\n        if isinstance(res_block_cls, str):\n            res_block_cls = eval(res_block_cls)\n        self.unet = Unet(\n            nfs=nfs,\n            n_blocks=n_blocks,\n            color_channels=color_channels,\n            act=act,\n            res_block_cls=res_block_cls,\n        )\n        if kaiming:\n            from slow_diffusion.init import kaiming as kaiming_\n\n            self.unet.apply(kaiming_)\n        self.save_hyperparameters()\n        self.loss_fn = torch.nn.MSELoss()\n\n    def step(self, batch):\n        (x_t, t), epsilon = batch\n        preds = self.unet(x_t, t)\n        return self.loss_fn(preds, epsilon)\n\n    def training_step(self, batch, batch_idx):\n        loss = self.step(batch)\n        self.log(\"train_loss\", loss, on_step=True, sync_dist=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss = self.step(batch)\n        self.log(\"test_loss\", loss, sync_dist=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.lr,\n            eps=self.hparams.adamw_epsilon,\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": torch.optim.lr_scheduler.OneCycleLR(\n                    optimizer,\n                    max_lr=self.hparams.lr,\n                    total_steps=self.trainer.estimated_stepping_batches,\n                    pct_start=self.hparams.one_cycle_pct_start,\n                    div_factor=self.hparams.one_cycle_div_factor,\n                    final_div_factor=self.hparams.one_cycle_final_div_factor,\n                ),\n                \"interval\": \"step\",\n                \"frequency\": 1,  # Update the LR every step\n                \"monitor\": \"test_loss\",  # Not relevant for OneCycleLR but specified anyways\n                \"strict\": True,  # FYI doesn't need to be strict because the monitor is irrelevant\n            },\n        }\n\n\nWe’ll use this for testing downstream.\n\nsource\n\n\nget_tiny_unet_lightning\n\n get_tiny_unet_lightning (**kwargs)\n\n\n\nExported source\ndef get_tiny_unet_lightning(**kwargs):\n    if \"act\" not in kwargs:\n        kwargs[\"act\"] = nn.ReLU\n    return UnetLightning(\n        nfs=[32, 64, 128, 256, 384],\n        n_blocks=[3, 2, 1, 1, 1, 1],\n        color_channels=1,\n        **kwargs,\n    )",
    "crumbs": [
      "Training Algorithm"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "source\n\n\n\n ᾱ (t, reshape=True)\n\n\n\nExported source\ndef ᾱ(t, reshape=True):\n    assert (0 &lt;= t).all() and (t &lt;= 1).all()\n    ᾱ_ = ((t * math.pi / 2).cos() ** 2).clamp(0.0, 0.999)\n    if reshape:\n        ᾱ_ = ᾱ_.reshape(-1, 1, 1, 1)\n    return ᾱ_\n\n\n\nsource\n\n\n\n\n noisify (x_0, t=None)\n\n\n\nExported source\ndef noisify(x_0, t=None):\n    n, *_ = x_0.shape\n    device = x_0.device\n\n    if t is None:\n        t = torch.rand((n,), device=device)\n\n    # Sample 2D noise for each example in the batch\n    ε = torch.randn(x_0.shape, device=device)\n\n    # Add noise according to the equation in Algorithm 1, such\n    # that the variance of the distribution does not change. Also,\n    # ensure that the overall magnitude does not change by 0-centering\n    # x_0 and 0.5-centering x_t\n    x_t = ᾱ(t).sqrt() * (x_0) + (1 - ᾱ(t)).sqrt() * ε\n\n    return ((x_t, t), ε)\n\n\n\nsource\n\n\n\n\n DiffusionDataModule (hf_ds_uri, bs, n_workers=0,\n                      img_size:tuple[int,int]|None=None,\n                      data_dir='./data')\n\nLightning DataModule wrapper for huggingface datasets. Helps with pre-processing the image data. Just add a noisify_fn(batch)!\n\n\nExported source\nclass DiffusionDataModule(L.LightningDataModule):\n    \"\"\"Lightning DataModule wrapper for huggingface datasets. Helps with\n    pre-processing the image data. Just add a noisify_fn(batch)!\"\"\"\n\n    def __init__(\n        self,\n        hf_ds_uri,\n        bs,\n        n_workers=0,\n        img_size: tuple[int, int] | None = None,\n        data_dir=\"./data\",\n    ):\n        super().__init__()\n        self.bs = bs\n        self.hf_ds_uri = hf_ds_uri\n        self.img_size = img_size\n        self._n_workers = n_workers\n        self.data_dir = Path(data_dir)\n\n    def noisify_fn(self, x_0):\n        # TODO: better to have subclasses provide a \"normalized x_0\" method\n        # and perform noisification consistent here in the super class...\n        raise NotImplementedError\n\n    def post_process(self, ds):\n        \"\"\"Optional post-processing pass after download but before freezing\"\"\"\n        raise NotImplementedError\n\n    def to_tensor(self, img):\n        if self.img_size is not None:\n            img = img.resize(self.img_size)\n        x = F.pil_to_tensor(img)\n        _, h, w = x.shape\n        assert h & (h - 1) == 0, f\"height ({h}) must be a power of two\"\n        assert w & (w - 1) == 0, f\"width ({w}) must be a power of two\"\n        return x\n\n    def _collate(self, batch):\n        x_0 = torch.stack([self.to_tensor(row[\"image\"]) for row in batch])\n        return self.noisify_fn(x_0)\n\n    def _freeze(self, batch):\n        x_0 = torch.stack([self.to_tensor(img) for img in batch[\"image\"]])\n        ((x_t, t), epsilon) = self.noisify_fn(x_0)\n        return {\"x_t\": x_t, \"t\": t, \"epsilon\": epsilon}\n\n    def _frozen_collate(self, rows):\n        def s(feature):\n            return torch.tensor([row[feature] for row in rows])\n\n        return (s(\"x_t\"), s(\"t\")), s(\"epsilon\")\n\n    @property\n    def cached_dir(self):\n        return self.data_dir / f\"{self.__class__.__name__}_{self.hf_ds_uri}\"\n\n    def clean(self):\n        self.cached_dir.unlink()\n\n    def setup(self, stage: str | None = None, test_splits=(\"test\",)):\n        if not self.cached_dir.exists():\n            ds = load_dataset(self.hf_ds_uri)\n\n            try:\n                ds = self.post_process(ds)\n            except NotImplementedError:\n                pass\n\n            for split in test_splits:\n                ds[split] = ds[split].map(\n                    self._freeze,\n                    batched=True,\n                    # We can discard the original data, as we only care about the noised information\n                    remove_columns=ds[split].features.keys(),\n                )\n\n            ds.save_to_disk(self.cached_dir)\n\n        # Load from disk to take advantage of mmapping\n        self.ds = load_from_disk(self.cached_dir)\n\n    @property\n    def n_workers(self):\n        if self._n_workers == -1:\n            return multiprocessing.cpu_count() - 1\n        else:\n            return self._n_workers\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.ds[\"train\"],\n            shuffle=True,\n            batch_size=self.bs,\n            collate_fn=self._collate,\n            num_workers=self.n_workers,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.ds[\"test\"],\n            batch_size=self.bs,\n            collate_fn=self._frozen_collate,\n            num_workers=self.n_workers,\n        )",
    "crumbs": [
      "Data"
    ]
  },
  {
    "objectID": "data.html#noisify",
    "href": "data.html#noisify",
    "title": "Data",
    "section": "",
    "text": "source\n\n\n\n ᾱ (t, reshape=True)\n\n\n\nExported source\ndef ᾱ(t, reshape=True):\n    assert (0 &lt;= t).all() and (t &lt;= 1).all()\n    ᾱ_ = ((t * math.pi / 2).cos() ** 2).clamp(0.0, 0.999)\n    if reshape:\n        ᾱ_ = ᾱ_.reshape(-1, 1, 1, 1)\n    return ᾱ_\n\n\n\nsource\n\n\n\n\n noisify (x_0, t=None)\n\n\n\nExported source\ndef noisify(x_0, t=None):\n    n, *_ = x_0.shape\n    device = x_0.device\n\n    if t is None:\n        t = torch.rand((n,), device=device)\n\n    # Sample 2D noise for each example in the batch\n    ε = torch.randn(x_0.shape, device=device)\n\n    # Add noise according to the equation in Algorithm 1, such\n    # that the variance of the distribution does not change. Also,\n    # ensure that the overall magnitude does not change by 0-centering\n    # x_0 and 0.5-centering x_t\n    x_t = ᾱ(t).sqrt() * (x_0) + (1 - ᾱ(t)).sqrt() * ε\n\n    return ((x_t, t), ε)\n\n\n\nsource\n\n\n\n\n DiffusionDataModule (hf_ds_uri, bs, n_workers=0,\n                      img_size:tuple[int,int]|None=None,\n                      data_dir='./data')\n\nLightning DataModule wrapper for huggingface datasets. Helps with pre-processing the image data. Just add a noisify_fn(batch)!\n\n\nExported source\nclass DiffusionDataModule(L.LightningDataModule):\n    \"\"\"Lightning DataModule wrapper for huggingface datasets. Helps with\n    pre-processing the image data. Just add a noisify_fn(batch)!\"\"\"\n\n    def __init__(\n        self,\n        hf_ds_uri,\n        bs,\n        n_workers=0,\n        img_size: tuple[int, int] | None = None,\n        data_dir=\"./data\",\n    ):\n        super().__init__()\n        self.bs = bs\n        self.hf_ds_uri = hf_ds_uri\n        self.img_size = img_size\n        self._n_workers = n_workers\n        self.data_dir = Path(data_dir)\n\n    def noisify_fn(self, x_0):\n        # TODO: better to have subclasses provide a \"normalized x_0\" method\n        # and perform noisification consistent here in the super class...\n        raise NotImplementedError\n\n    def post_process(self, ds):\n        \"\"\"Optional post-processing pass after download but before freezing\"\"\"\n        raise NotImplementedError\n\n    def to_tensor(self, img):\n        if self.img_size is not None:\n            img = img.resize(self.img_size)\n        x = F.pil_to_tensor(img)\n        _, h, w = x.shape\n        assert h & (h - 1) == 0, f\"height ({h}) must be a power of two\"\n        assert w & (w - 1) == 0, f\"width ({w}) must be a power of two\"\n        return x\n\n    def _collate(self, batch):\n        x_0 = torch.stack([self.to_tensor(row[\"image\"]) for row in batch])\n        return self.noisify_fn(x_0)\n\n    def _freeze(self, batch):\n        x_0 = torch.stack([self.to_tensor(img) for img in batch[\"image\"]])\n        ((x_t, t), epsilon) = self.noisify_fn(x_0)\n        return {\"x_t\": x_t, \"t\": t, \"epsilon\": epsilon}\n\n    def _frozen_collate(self, rows):\n        def s(feature):\n            return torch.tensor([row[feature] for row in rows])\n\n        return (s(\"x_t\"), s(\"t\")), s(\"epsilon\")\n\n    @property\n    def cached_dir(self):\n        return self.data_dir / f\"{self.__class__.__name__}_{self.hf_ds_uri}\"\n\n    def clean(self):\n        self.cached_dir.unlink()\n\n    def setup(self, stage: str | None = None, test_splits=(\"test\",)):\n        if not self.cached_dir.exists():\n            ds = load_dataset(self.hf_ds_uri)\n\n            try:\n                ds = self.post_process(ds)\n            except NotImplementedError:\n                pass\n\n            for split in test_splits:\n                ds[split] = ds[split].map(\n                    self._freeze,\n                    batched=True,\n                    # We can discard the original data, as we only care about the noised information\n                    remove_columns=ds[split].features.keys(),\n                )\n\n            ds.save_to_disk(self.cached_dir)\n\n        # Load from disk to take advantage of mmapping\n        self.ds = load_from_disk(self.cached_dir)\n\n    @property\n    def n_workers(self):\n        if self._n_workers == -1:\n            return multiprocessing.cpu_count() - 1\n        else:\n            return self._n_workers\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.ds[\"train\"],\n            shuffle=True,\n            batch_size=self.bs,\n            collate_fn=self._collate,\n            num_workers=self.n_workers,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.ds[\"test\"],\n            batch_size=self.bs,\n            collate_fn=self._frozen_collate,\n            num_workers=self.n_workers,\n        )",
    "crumbs": [
      "Data"
    ]
  }
]